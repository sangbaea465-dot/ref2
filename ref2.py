import os
import streamlit as st
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì œê±°: API KeyëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥ë°›ì•„ ì‚¬ìš©

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ê¸°ë°˜ RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì´ˆê¸° ìƒíƒœ ì„¤ì •
if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "processed_files" not in st.session_state:
    st.session_state.processed_files = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# API Key ìƒíƒœ
if "api_key" not in st.session_state:
    st.session_state.api_key = ""

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
/* í—¤ë”© ìŠ¤íƒ€ì¼ */
h1 {
    font-size: 1.4rem !important;
    font-weight: 600 !important;
    color: #ff69b4 !important; /* ë¶„í™ìƒ‰ */
}
h2 {
    font-size: 1.2rem !important;
    font-weight: 600 !important;
    color: #ffd700 !important; /* ë…¸ë‘ìƒ‰ */
}
h3 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
    color: #1f77b4 !important; /* ì²­ìƒ‰ */
}
h4 {
    font-size: 1.1rem !important;
    font-weight: 600 !important;
}
h5 {
    font-size: 1rem !important;
    font-weight: 600 !important;
}
h6 {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
.stChatMessage {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
}

/* ë‹µë³€ ë‚´ìš© ìŠ¤íƒ€ì¼ */
.stChatMessage p {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

/* ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage ul, .stChatMessage ol {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
}

.stChatMessage li {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.3rem 0 !important;
}

/* ê°•ì¡° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ */
.stChatMessage strong, .stChatMessage b {
    font-size: 0.95rem !important;
    font-weight: 600 !important;
}

/* ì¸ìš©ë¬¸ ìŠ¤íƒ€ì¼ */
.stChatMessage blockquote {
    font-size: 0.95rem !important;
    line-height: 1.5 !important;
    margin: 0.5rem 0 !important;
    padding-left: 1rem !important;
    border-left: 3px solid #e0e0e0 !important;
}

/* ì½”ë“œ ìŠ¤íƒ€ì¼ */
.stChatMessage code {
    font-size: 0.9rem !important;
    background-color: #f5f5f5 !important;
    padding: 0.2rem 0.4rem !important;
    border-radius: 3px !important;
}

/* ì „ì²´ í…ìŠ¤íŠ¸ ì¼ê´€ì„± */
.stChatMessage * {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;
}

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.stButton > button {
    background-color: #ff69b4 !important;
    color: white !important;
    border: none !important;
    border-radius: 5px !important;
    padding: 0.5rem 1rem !important;
    font-weight: bold !important;
}

.stButton > button:hover {
    background-color: #ff1493 !important;
}
</style>
""", unsafe_allow_html=True)

# ì œëª©
st.markdown("""
<div style="text-align: center; margin-top: -4rem; margin-bottom: 0.5rem;">
    <h1 style="font-size: 2.5rem; font-weight: bold; margin: 0;">
        <span style="color: #1f77b4;">PDF</span> 
        <span style="color: #ffffff; font-size: 0.7em;">ê¸°ë°˜</span> 
        <span style="color: #ffd700;">RAG</span> 
        <span style="color: #d62728; font-size: 0.7em;">ì±—ë´‡</span>
    </h1>

</div>
""", unsafe_allow_html=True)

st.markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë‚´ìš©ì— ê´€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.markdown('<h2 style="color: #1f77b4;">OpenAI API Key</h2>', unsafe_allow_html=True)
    api_key_input = st.text_input("API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", value=st.session_state.api_key)
    if api_key_input != st.session_state.api_key:
        st.session_state.api_key = api_key_input
    has_api_key = bool(st.session_state.api_key)

    if not has_api_key:
        st.info("API Keyë¥¼ ì…ë ¥í•˜ë©´ íŒŒì¼ ì²˜ë¦¬ ë° ì±„íŒ…ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    st.markdown('<h2 style="color: #1f77b4;">PDF íŒŒì¼ ì—…ë¡œë“œ</h2>', unsafe_allow_html=True)
    uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        process_button = st.button("íŒŒì¼ ì²˜ë¦¬í•˜ê¸°", disabled=not has_api_key)
        
        if process_button:
            with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì²˜ë¦¬
                    temp_dir = tempfile.TemporaryDirectory()
                    
                    all_docs = []
                    new_files = []
                    
                    # ê° íŒŒì¼ ì²˜ë¦¬
                    for uploaded_file in uploaded_files:
                        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ
                        if uploaded_file.name in st.session_state.processed_files:
                            continue
                            
                        temp_file_path = os.path.join(temp_dir.name, uploaded_file.name)
                        
                        # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                        with open(temp_file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        # PDF ë¡œë” ìƒì„± ë° ë¬¸ì„œ ë¡œë“œ
                        loader = PyPDFLoader(temp_file_path)
                        documents = loader.load()
                        
                        # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì´ë¦„ ì¶”ê°€
                        for doc in documents:
                            doc.metadata["source"] = uploaded_file.name
                        
                        all_docs.extend(documents)
                        new_files.append(uploaded_file.name)
                
                    if not all_docs:
                        st.success("ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=500,
                            chunk_overlap=100,
                            length_function=len
                        )
                        chunks = text_splitter.split_documents(all_docs)
                        
                        # ëª¨ë“  ì²­í¬ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        total_chunks = len(chunks)
                        st.info(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                        
                        # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì‚¬ì´ë“œë°” API Key ì‚¬ìš©)
                        embeddings = OpenAIEmbeddings(api_key=st.session_state.api_key)
                        
                        if st.session_state.vectorstore is None:
                            # ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                            batch_size = 30
                            vectorstore = None
                            
                            for i in range(0, len(chunks), batch_size):
                                batch_chunks = chunks[i:i + batch_size]
                                
                                try:
                                    if vectorstore is None:
                                        vectorstore = FAISS.from_documents(batch_chunks, embeddings)
                                    else:
                                        vectorstore.add_documents(batch_chunks)
                                except Exception as e:
                                    continue
                            
                            st.session_state.vectorstore = vectorstore
                        else:
                            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
                            batch_size = 30
                            
                            for i in range(0, len(chunks), batch_size):
                                batch_chunks = chunks[i:i + batch_size]
                                
                                try:
                                    st.session_state.vectorstore.add_documents(batch_chunks)
                                except Exception as e:
                                    continue
                        
                        # ê²€ìƒ‰ê¸° ìƒì„± (ë” ë§ì€ ê²°ê³¼ì™€ ì •í™•í•œ ê²€ìƒ‰)
                        st.session_state.retriever = st.session_state.vectorstore.as_retriever(
                            search_type="similarity",
                            search_kwargs={"k": 10}  # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¦ê°€
                        )
                        
                        # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸
                        st.session_state.processed_files.extend(new_files)
                        
                except Exception as e:
                    st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.error("íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    if st.session_state.processed_files:
        st.markdown('<h3 style="color: #ffd700;">ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡</h3>', unsafe_allow_html=True)
        for file in st.session_state.processed_files:
            st.write(f"- {file}")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.session_state.conversation_memory = []
        st.rerun()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
    if st.session_state.processed_files:
        st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        st.info(f"ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(st.session_state.processed_files)}")
        st.info(f"ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(st.session_state.chat_history)}")

# ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì˜ì—­
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", disabled=not bool(st.session_state.api_key)):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.write(prompt)
    
    if st.session_state.retriever is None:
        with st.chat_message("assistant"):
            st.write("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
        st.session_state.chat_history.append({"role": "assistant", "content": "ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."})
    else:
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                if not st.session_state.api_key:
                    raise ValueError("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
                # RAG ê²€ìƒ‰ (ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©)
                retrieved_docs = st.session_state.retriever.invoke(prompt)
                
                if not retrieved_docs:
                    response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{prompt}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                else:
                    # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                    top_docs = retrieved_docs[:3]
                    
                    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    context_text = ""
                    max_context_length = 8000
                    current_length = 0
                    
                    for i, doc in enumerate(top_docs):
                        doc_text = f"[ë¬¸ì„œ {i+1}]\n{doc.page_content}\n\n"
                        if current_length + len(doc_text) > max_context_length:
                            st.warning(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë¬¸ì„œ {i+1}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            break
                        context_text += doc_text
                        current_length += len(doc_text)
                    
                    # ê³¼ê±° ëŒ€í™” ë§¥ë½ êµ¬ì„±
                    conversation_context = ""
                    if st.session_state.conversation_memory:
                        conversation_context = "\n\n=== ì´ì „ ëŒ€í™” ë§¥ë½ ===\n"
                        # ìµœê·¼ 50ê°œ ëŒ€í™” ì‚¬ìš©
                        recent_conversations = st.session_state.conversation_memory[-50:]
                        for conv in recent_conversations:
                            conversation_context += f"{conv}\n"
                        conversation_context += "=== ëŒ€í™” ë§¥ë½ ë ===\n"
                    
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    system_prompt = f"""
                    ì§ˆë¬¸: {prompt}
                    
                    ê´€ë ¨ ë¬¸ì„œ:
                    {context_text}{conversation_context}
                    
                    ìœ„ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
                    ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì°¸ì¡°í•˜ì—¬ ë” ì •í™•í•˜ê³  ë§¥ë½ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                    
                    ë‹µë³€ í˜•ì‹:
                    - ë‹µë³€ì€ ë°˜ë“œì‹œ í—¤ë”©(# ## ###)ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”
                    - ì£¼ìš” ì£¼ì œëŠ” # (H1)ë¡œ, ì„¸ë¶€ ë‚´ìš©ì€ ## (H2)ë¡œ, êµ¬ì²´ì  ì„¤ëª…ì€ ### (H3)ë¡œ êµ¬ë¶„í•˜ì„¸ìš”
                    - ë‹µë³€ì´ ê¸¸ê±°ë‚˜ ë³µì¡í•œ ê²½ìš° ì—¬ëŸ¬ í—¤ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
                    - ë‹µë³€ì€ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ë˜ ì¡´ëŒ€ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”
                    - ê°œì¡°ì‹ì´ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”
                    
                    ì£¼ì˜ì‚¬í•­:
                    - ë‹µë³€ ì¤‘ê°„ì— (ë¬¸ì„œ1), (ë¬¸ì„œ2) ê°™ì€ ì°¸ì¡° í‘œì‹œë¥¼ í•˜ì§€ ë§ˆì„¸ìš”
                    - "ì°¸ì¡° ë¬¸ì„œ:", "ì œê³µëœ ë¬¸ì„œ", "ë¬¸ì„œ 1, ë¬¸ì„œ 2" ê°™ì€ ë¬¸êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
                    - ë‹µë³€ì€ ìˆœìˆ˜í•œ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³ , ì°¸ì¡° ê´€ë ¨ ë¬¸êµ¬ëŠ” ì „í˜€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
                    - ë‹µë³€ ëì— ì°¸ì¡° ì •ë³´ë‚˜ ì¶œì²˜ ê´€ë ¨ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
                    """
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì‚¬ì´ë“œë°” API Key ì‚¬ìš©)
                    llm = ChatOpenAI(model="gpt-4o-mini", temperature=1, api_key=st.session_state.api_key)
                    response = llm.invoke(system_prompt).content
                    
                
                # ë‹µë³€ í‘œì‹œ
                with st.chat_message("assistant"):
                    st.write(response)
                
                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                
                # ëŒ€í™” ë§¥ë½ ë©”ëª¨ë¦¬ì— ì¶”ê°€ (ìµœê·¼ 50ê°œ ëŒ€í™” ìœ ì§€)
                st.session_state.conversation_memory.append(f"ì‚¬ìš©ì: {prompt}")
                st.session_state.conversation_memory.append(f"AI: {response}")
                if len(st.session_state.conversation_memory) > 100:  # 50ê°œ ëŒ€í™” = 100ê°œ ë©”ì‹œì§€
                    st.session_state.conversation_memory = st.session_state.conversation_memory[-100:]
                
            except Exception as e:
                with st.chat_message("assistant"):
                    st.write(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.session_state.chat_history.append({"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"})


